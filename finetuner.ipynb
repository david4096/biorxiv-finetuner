{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e346537-4f2b-4b8d-9dbc-594e450e0138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 82472.27it/s]\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.033% (0.819M/2506.172M)\n",
      "Starting training..., iters: 1000\n",
      "Iter 1: Val loss 3.683, Val took 39.976s\n",
      "Iter 10: Train loss 3.588, Learning Rate 1.000e-05, It/sec 0.247, Tokens/sec 245.479, Trained Tokens 9929, Peak mem 17.467 GB\n",
      "Iter 20: Train loss 3.239, Learning Rate 1.000e-05, It/sec 0.251, Tokens/sec 222.129, Trained Tokens 18766, Peak mem 17.467 GB\n",
      "Iter 30: Train loss 3.386, Learning Rate 1.000e-05, It/sec 0.267, Tokens/sec 223.801, Trained Tokens 27143, Peak mem 17.467 GB\n",
      "Iter 40: Train loss 3.168, Learning Rate 1.000e-05, It/sec 0.288, Tokens/sec 228.044, Trained Tokens 35052, Peak mem 17.467 GB\n",
      "Iter 50: Train loss 2.912, Learning Rate 1.000e-05, It/sec 0.197, Tokens/sec 207.684, Trained Tokens 45591, Peak mem 17.467 GB\n",
      "Iter 60: Train loss 3.281, Learning Rate 1.000e-05, It/sec 0.298, Tokens/sec 242.184, Trained Tokens 53706, Peak mem 17.467 GB\n",
      "Iter 70: Train loss 3.076, Learning Rate 1.000e-05, It/sec 0.269, Tokens/sec 245.589, Trained Tokens 62826, Peak mem 17.467 GB\n",
      "Iter 80: Train loss 3.090, Learning Rate 1.000e-05, It/sec 0.239, Tokens/sec 218.310, Trained Tokens 71973, Peak mem 21.063 GB\n",
      "Iter 90: Train loss 3.035, Learning Rate 1.000e-05, It/sec 0.253, Tokens/sec 213.062, Trained Tokens 80401, Peak mem 21.063 GB\n",
      "Iter 100: Train loss 3.181, Learning Rate 1.000e-05, It/sec 0.243, Tokens/sec 191.829, Trained Tokens 88290, Peak mem 21.063 GB\n",
      "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 3.013, Learning Rate 1.000e-05, It/sec 0.242, Tokens/sec 202.638, Trained Tokens 96668, Peak mem 21.063 GB\n",
      "Iter 120: Train loss 3.170, Learning Rate 1.000e-05, It/sec 0.196, Tokens/sec 181.281, Trained Tokens 105927, Peak mem 21.063 GB\n",
      "Iter 130: Train loss 2.863, Learning Rate 1.000e-05, It/sec 0.174, Tokens/sec 184.521, Trained Tokens 116529, Peak mem 21.063 GB\n",
      "Iter 140: Train loss 3.116, Learning Rate 1.000e-05, It/sec 0.235, Tokens/sec 215.872, Trained Tokens 125699, Peak mem 21.063 GB\n",
      "Iter 150: Train loss 3.097, Learning Rate 1.000e-05, It/sec 0.248, Tokens/sec 198.438, Trained Tokens 133700, Peak mem 21.063 GB\n",
      "Iter 160: Train loss 3.112, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 165.642, Trained Tokens 142785, Peak mem 21.063 GB\n",
      "Iter 170: Train loss 2.887, Learning Rate 1.000e-05, It/sec 0.161, Tokens/sec 181.666, Trained Tokens 154097, Peak mem 21.063 GB\n",
      "Iter 180: Train loss 3.033, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 174.622, Trained Tokens 163449, Peak mem 21.063 GB\n",
      "Iter 190: Train loss 3.101, Learning Rate 1.000e-05, It/sec 0.220, Tokens/sec 195.412, Trained Tokens 172342, Peak mem 21.063 GB\n",
      "Iter 200: Val loss 3.013, Val took 42.013s\n",
      "Iter 200: Train loss 3.067, Learning Rate 1.000e-05, It/sec 2.471, Tokens/sec 2569.639, Trained Tokens 182743, Peak mem 21.063 GB\n",
      "Iter 200: Saved adapter weights to adapters/adapters.safetensors and adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 3.024, Learning Rate 1.000e-05, It/sec 0.254, Tokens/sec 244.115, Trained Tokens 192337, Peak mem 27.984 GB\n",
      "Iter 220: Train loss 2.701, Learning Rate 1.000e-05, It/sec 0.179, Tokens/sec 189.774, Trained Tokens 202911, Peak mem 27.984 GB\n",
      "Iter 230: Train loss 3.207, Learning Rate 1.000e-05, It/sec 0.265, Tokens/sec 236.157, Trained Tokens 211818, Peak mem 27.984 GB\n",
      "Iter 240: Train loss 2.996, Learning Rate 1.000e-05, It/sec 0.233, Tokens/sec 212.223, Trained Tokens 220936, Peak mem 27.984 GB\n",
      "Iter 250: Train loss 3.133, Learning Rate 1.000e-05, It/sec 0.254, Tokens/sec 215.241, Trained Tokens 229422, Peak mem 27.984 GB\n",
      "Iter 260: Train loss 3.172, Learning Rate 1.000e-05, It/sec 0.246, Tokens/sec 211.971, Trained Tokens 238043, Peak mem 27.984 GB\n",
      "Iter 270: Train loss 3.132, Learning Rate 1.000e-05, It/sec 0.227, Tokens/sec 201.636, Trained Tokens 246927, Peak mem 27.984 GB\n",
      "Iter 280: Train loss 3.109, Learning Rate 1.000e-05, It/sec 0.232, Tokens/sec 202.279, Trained Tokens 255644, Peak mem 27.984 GB\n",
      "Iter 290: Train loss 2.918, Learning Rate 1.000e-05, It/sec 0.207, Tokens/sec 224.892, Trained Tokens 266517, Peak mem 27.984 GB\n",
      "Iter 300: Train loss 3.007, Learning Rate 1.000e-05, It/sec 0.244, Tokens/sec 240.636, Trained Tokens 276393, Peak mem 27.984 GB\n",
      "Iter 300: Saved adapter weights to adapters/adapters.safetensors and adapters/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 2.979, Learning Rate 1.000e-05, It/sec 0.237, Tokens/sec 218.919, Trained Tokens 285631, Peak mem 27.984 GB\n",
      "Iter 320: Train loss 2.902, Learning Rate 1.000e-05, It/sec 0.225, Tokens/sec 212.677, Trained Tokens 295089, Peak mem 27.984 GB\n",
      "Iter 330: Train loss 3.136, Learning Rate 1.000e-05, It/sec 0.289, Tokens/sec 238.496, Trained Tokens 303344, Peak mem 27.984 GB\n",
      "Iter 340: Train loss 2.789, Learning Rate 1.000e-05, It/sec 0.199, Tokens/sec 204.870, Trained Tokens 313622, Peak mem 27.984 GB\n",
      "Iter 350: Train loss 3.276, Learning Rate 1.000e-05, It/sec 0.225, Tokens/sec 197.921, Trained Tokens 322411, Peak mem 27.984 GB\n",
      "Iter 360: Train loss 2.893, Learning Rate 1.000e-05, It/sec 0.277, Tokens/sec 228.675, Trained Tokens 330660, Peak mem 27.984 GB\n",
      "Iter 370: Train loss 2.755, Learning Rate 1.000e-05, It/sec 0.223, Tokens/sec 216.827, Trained Tokens 340396, Peak mem 27.984 GB\n",
      "Iter 380: Train loss 3.106, Learning Rate 1.000e-05, It/sec 0.253, Tokens/sec 201.190, Trained Tokens 348344, Peak mem 27.984 GB\n",
      "Iter 390: Train loss 2.857, Learning Rate 1.000e-05, It/sec 0.197, Tokens/sec 171.140, Trained Tokens 357019, Peak mem 27.984 GB\n",
      "Iter 400: Val loss 3.003, Val took 42.398s\n",
      "Iter 400: Train loss 2.749, Learning Rate 1.000e-05, It/sec 1.648, Tokens/sec 1532.333, Trained Tokens 366316, Peak mem 27.984 GB\n",
      "Iter 400: Saved adapter weights to adapters/adapters.safetensors and adapters/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 3.218, Learning Rate 1.000e-05, It/sec 0.274, Tokens/sec 227.346, Trained Tokens 374601, Peak mem 27.984 GB\n",
      "Iter 420: Train loss 2.997, Learning Rate 1.000e-05, It/sec 0.241, Tokens/sec 196.486, Trained Tokens 382738, Peak mem 27.984 GB\n",
      "Iter 430: Train loss 2.974, Learning Rate 1.000e-05, It/sec 0.215, Tokens/sec 193.424, Trained Tokens 391725, Peak mem 27.984 GB\n",
      "Iter 440: Train loss 3.134, Learning Rate 1.000e-05, It/sec 0.279, Tokens/sec 224.884, Trained Tokens 399774, Peak mem 27.984 GB\n",
      "Iter 450: Train loss 2.850, Learning Rate 1.000e-05, It/sec 0.222, Tokens/sec 203.007, Trained Tokens 408938, Peak mem 27.984 GB\n",
      "Iter 460: Train loss 3.034, Learning Rate 1.000e-05, It/sec 0.237, Tokens/sec 199.426, Trained Tokens 417341, Peak mem 27.984 GB\n",
      "Iter 470: Train loss 3.191, Learning Rate 1.000e-05, It/sec 0.252, Tokens/sec 222.917, Trained Tokens 426177, Peak mem 27.984 GB\n",
      "Iter 480: Train loss 2.917, Learning Rate 1.000e-05, It/sec 0.264, Tokens/sec 244.831, Trained Tokens 435467, Peak mem 27.984 GB\n",
      "Iter 490: Train loss 3.107, Learning Rate 1.000e-05, It/sec 0.258, Tokens/sec 199.856, Trained Tokens 443219, Peak mem 27.984 GB\n",
      "Iter 500: Train loss 3.077, Learning Rate 1.000e-05, It/sec 0.199, Tokens/sec 193.083, Trained Tokens 452930, Peak mem 27.984 GB\n",
      "Iter 500: Saved adapter weights to adapters/adapters.safetensors and adapters/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 2.769, Learning Rate 1.000e-05, It/sec 0.196, Tokens/sec 179.675, Trained Tokens 462080, Peak mem 27.984 GB\n",
      "Iter 520: Train loss 3.119, Learning Rate 1.000e-05, It/sec 0.293, Tokens/sec 230.574, Trained Tokens 469952, Peak mem 27.984 GB\n",
      "Iter 530: Train loss 3.079, Learning Rate 1.000e-05, It/sec 0.244, Tokens/sec 211.716, Trained Tokens 478623, Peak mem 27.984 GB\n",
      "Iter 540: Train loss 2.965, Learning Rate 1.000e-05, It/sec 0.226, Tokens/sec 197.533, Trained Tokens 487346, Peak mem 27.984 GB\n",
      "Iter 550: Train loss 3.240, Learning Rate 1.000e-05, It/sec 0.297, Tokens/sec 228.328, Trained Tokens 495041, Peak mem 27.984 GB\n",
      "Iter 560: Train loss 2.932, Learning Rate 1.000e-05, It/sec 0.255, Tokens/sec 200.047, Trained Tokens 502890, Peak mem 27.984 GB\n",
      "Iter 570: Train loss 3.068, Learning Rate 1.000e-05, It/sec 0.199, Tokens/sec 196.283, Trained Tokens 512741, Peak mem 27.984 GB\n",
      "Iter 580: Train loss 3.105, Learning Rate 1.000e-05, It/sec 0.224, Tokens/sec 177.564, Trained Tokens 520685, Peak mem 27.984 GB\n",
      "Iter 590: Train loss 3.118, Learning Rate 1.000e-05, It/sec 0.250, Tokens/sec 206.776, Trained Tokens 528972, Peak mem 27.984 GB\n",
      "Iter 600: Val loss 2.867, Val took 36.733s\n",
      "Iter 600: Train loss 2.926, Learning Rate 1.000e-05, It/sec 3.633, Tokens/sec 3317.597, Trained Tokens 538104, Peak mem 27.984 GB\n",
      "Iter 600: Saved adapter weights to adapters/adapters.safetensors and adapters/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 2.793, Learning Rate 1.000e-05, It/sec 0.267, Tokens/sec 270.806, Trained Tokens 548257, Peak mem 27.984 GB\n",
      "Iter 620: Train loss 2.841, Learning Rate 1.000e-05, It/sec 0.221, Tokens/sec 206.974, Trained Tokens 557622, Peak mem 27.984 GB\n",
      "Iter 630: Train loss 3.051, Learning Rate 1.000e-05, It/sec 0.260, Tokens/sec 211.886, Trained Tokens 565764, Peak mem 27.984 GB\n",
      "Iter 640: Train loss 2.977, Learning Rate 1.000e-05, It/sec 0.250, Tokens/sec 216.029, Trained Tokens 574390, Peak mem 27.984 GB\n",
      "Iter 650: Train loss 2.868, Learning Rate 1.000e-05, It/sec 0.189, Tokens/sec 190.308, Trained Tokens 584452, Peak mem 27.984 GB\n",
      "Iter 660: Train loss 2.905, Learning Rate 1.000e-05, It/sec 0.245, Tokens/sec 221.757, Trained Tokens 593488, Peak mem 27.984 GB\n",
      "Iter 670: Train loss 3.040, Learning Rate 1.000e-05, It/sec 0.228, Tokens/sec 205.566, Trained Tokens 602486, Peak mem 27.984 GB\n",
      "Iter 680: Train loss 2.894, Learning Rate 1.000e-05, It/sec 0.208, Tokens/sec 190.882, Trained Tokens 611672, Peak mem 27.984 GB\n",
      "Iter 690: Train loss 2.970, Learning Rate 1.000e-05, It/sec 0.226, Tokens/sec 213.739, Trained Tokens 621149, Peak mem 27.984 GB\n",
      "Iter 700: Train loss 3.047, Learning Rate 1.000e-05, It/sec 0.273, Tokens/sec 218.506, Trained Tokens 629144, Peak mem 27.984 GB\n",
      "Iter 700: Saved adapter weights to adapters/adapters.safetensors and adapters/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 2.964, Learning Rate 1.000e-05, It/sec 0.296, Tokens/sec 221.762, Trained Tokens 636647, Peak mem 27.984 GB\n",
      "Iter 720: Train loss 2.894, Learning Rate 1.000e-05, It/sec 0.194, Tokens/sec 180.194, Trained Tokens 645938, Peak mem 27.984 GB\n",
      "Iter 730: Train loss 2.863, Learning Rate 1.000e-05, It/sec 0.204, Tokens/sec 190.007, Trained Tokens 655234, Peak mem 27.984 GB\n",
      "Iter 740: Train loss 3.036, Learning Rate 1.000e-05, It/sec 0.278, Tokens/sec 219.166, Trained Tokens 663120, Peak mem 27.984 GB\n",
      "Iter 750: Train loss 2.776, Learning Rate 1.000e-05, It/sec 0.209, Tokens/sec 192.747, Trained Tokens 672354, Peak mem 27.984 GB\n",
      "Iter 760: Train loss 2.783, Learning Rate 1.000e-05, It/sec 0.180, Tokens/sec 167.110, Trained Tokens 681630, Peak mem 27.984 GB\n",
      "Iter 770: Train loss 2.950, Learning Rate 1.000e-05, It/sec 0.230, Tokens/sec 198.580, Trained Tokens 690259, Peak mem 27.984 GB\n",
      "Iter 780: Train loss 3.169, Learning Rate 1.000e-05, It/sec 0.255, Tokens/sec 211.924, Trained Tokens 698577, Peak mem 27.984 GB\n",
      "Iter 790: Train loss 2.989, Learning Rate 1.000e-05, It/sec 0.291, Tokens/sec 224.201, Trained Tokens 706286, Peak mem 27.984 GB\n",
      "Iter 800: Val loss 2.797, Val took 49.186s\n",
      "Iter 800: Train loss 3.193, Learning Rate 1.000e-05, It/sec 4.682, Tokens/sec 3622.317, Trained Tokens 714022, Peak mem 27.984 GB\n",
      "Iter 800: Saved adapter weights to adapters/adapters.safetensors and adapters/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 2.899, Learning Rate 1.000e-05, It/sec 0.273, Tokens/sec 257.903, Trained Tokens 723456, Peak mem 27.984 GB\n",
      "Iter 820: Train loss 3.122, Learning Rate 1.000e-05, It/sec 0.225, Tokens/sec 207.200, Trained Tokens 732676, Peak mem 27.984 GB\n",
      "Iter 830: Train loss 2.639, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 201.173, Trained Tokens 744292, Peak mem 27.984 GB\n",
      "Iter 840: Train loss 3.092, Learning Rate 1.000e-05, It/sec 0.261, Tokens/sec 218.144, Trained Tokens 752650, Peak mem 27.984 GB\n",
      "Iter 850: Train loss 3.008, Learning Rate 1.000e-05, It/sec 0.301, Tokens/sec 243.367, Trained Tokens 760735, Peak mem 27.984 GB\n",
      "Iter 860: Train loss 2.881, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 176.729, Trained Tokens 770464, Peak mem 27.984 GB\n",
      "Iter 870: Train loss 2.933, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 190.796, Trained Tokens 780723, Peak mem 27.984 GB\n",
      "Iter 880: Train loss 3.074, Learning Rate 1.000e-05, It/sec 0.217, Tokens/sec 186.928, Trained Tokens 789342, Peak mem 27.984 GB\n",
      "Iter 890: Train loss 2.801, Learning Rate 1.000e-05, It/sec 0.197, Tokens/sec 188.799, Trained Tokens 798940, Peak mem 27.984 GB\n",
      "Iter 900: Train loss 3.215, Learning Rate 1.000e-05, It/sec 0.231, Tokens/sec 180.015, Trained Tokens 806722, Peak mem 27.984 GB\n",
      "Iter 900: Saved adapter weights to adapters/adapters.safetensors and adapters/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 2.934, Learning Rate 1.000e-05, It/sec 0.256, Tokens/sec 209.628, Trained Tokens 814903, Peak mem 27.984 GB\n",
      "Iter 920: Train loss 2.725, Learning Rate 1.000e-05, It/sec 0.175, Tokens/sec 184.194, Trained Tokens 825430, Peak mem 27.984 GB\n",
      "Iter 930: Train loss 2.873, Learning Rate 1.000e-05, It/sec 0.189, Tokens/sec 164.389, Trained Tokens 834118, Peak mem 27.984 GB\n",
      "Iter 940: Train loss 2.775, Learning Rate 1.000e-05, It/sec 0.181, Tokens/sec 192.898, Trained Tokens 844801, Peak mem 27.984 GB\n",
      "Iter 950: Train loss 3.199, Learning Rate 1.000e-05, It/sec 0.227, Tokens/sec 194.955, Trained Tokens 853396, Peak mem 27.984 GB\n",
      "Iter 960: Train loss 2.778, Learning Rate 1.000e-05, It/sec 0.197, Tokens/sec 189.942, Trained Tokens 863055, Peak mem 27.984 GB\n",
      "Iter 970: Train loss 2.859, Learning Rate 1.000e-05, It/sec 0.216, Tokens/sec 194.054, Trained Tokens 872055, Peak mem 27.984 GB\n",
      "Iter 980: Train loss 2.828, Learning Rate 1.000e-05, It/sec 0.175, Tokens/sec 182.122, Trained Tokens 882437, Peak mem 27.984 GB\n",
      "Iter 990: Train loss 2.828, Learning Rate 1.000e-05, It/sec 0.219, Tokens/sec 196.095, Trained Tokens 891379, Peak mem 27.984 GB\n",
      "Iter 1000: Val loss 2.833, Val took 38.556s\n",
      "Iter 1000: Train loss 2.928, Learning Rate 1.000e-05, It/sec 4.230, Tokens/sec 3777.881, Trained Tokens 900310, Peak mem 27.984 GB\n",
      "Iter 1000: Saved adapter weights to adapters/adapters.safetensors and adapters/0001000_adapters.safetensors.\n",
      "Saved final adapter weights to adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.lora \\\n",
    "  --train \\\n",
    "  --model mlx-community/quantized-gemma-2b-it \\\n",
    "  --data new/splits \\\n",
    "  --batch-size 2 \\\n",
    "  --lora-layers 16 \\\n",
    "  --iters 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85d891-73a1-44bd-abf7-fbc6ee6eb9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01680440-6413-4827-bd77-b86bc8476ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 84536.36it/s]\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.005% (3.277M/70553.706M)\n",
      "Starting training..., iters: 1337\n",
      "Iter 1: Val loss 1.878, Val took 3363.510s\n",
      "Iter 10: Train loss 2.080, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.479, Trained Tokens 9422, Peak mem 50.784 GB\n",
      "Iter 20: Train loss 1.954, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.866, Trained Tokens 17968, Peak mem 50.784 GB\n",
      "Iter 30: Train loss 2.226, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.750, Trained Tokens 26255, Peak mem 50.784 GB\n",
      "Iter 40: Train loss 2.142, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.468, Trained Tokens 34099, Peak mem 50.784 GB\n",
      "Iter 50: Train loss 1.864, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.507, Trained Tokens 44103, Peak mem 50.784 GB\n",
      "Iter 60: Train loss 2.193, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.696, Trained Tokens 52152, Peak mem 50.784 GB\n",
      "Iter 70: Train loss 2.041, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.334, Trained Tokens 61129, Peak mem 50.784 GB\n",
      "Iter 80: Train loss 2.019, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.164, Trained Tokens 69993, Peak mem 54.291 GB\n",
      "Iter 90: Train loss 1.987, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.862, Trained Tokens 78292, Peak mem 54.291 GB\n",
      "Iter 100: Train loss 2.029, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.486, Trained Tokens 86095, Peak mem 54.291 GB\n",
      "Iter 100: Saved adapter weights to llama-adapter-70/adapters.safetensors and llama-adapter-70/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 1.952, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.770, Trained Tokens 94380, Peak mem 54.291 GB\n",
      "Iter 120: Train loss 1.911, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.331, Trained Tokens 103543, Peak mem 54.291 GB\n",
      "Iter 130: Train loss 1.643, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.854, Trained Tokens 113745, Peak mem 54.291 GB\n",
      "Iter 140: Train loss 2.063, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.093, Trained Tokens 122580, Peak mem 54.291 GB\n",
      "Iter 150: Train loss 2.063, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.520, Trained Tokens 130526, Peak mem 54.291 GB\n",
      "Iter 160: Train loss 2.002, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.025, Trained Tokens 139459, Peak mem 54.291 GB\n",
      "Iter 170: Train loss 1.622, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 7.269, Trained Tokens 150347, Peak mem 54.291 GB\n",
      "Iter 180: Train loss 1.890, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.964, Trained Tokens 159294, Peak mem 54.291 GB\n",
      "Iter 190: Train loss 1.983, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.018, Trained Tokens 168001, Peak mem 54.291 GB\n",
      "Iter 200: Val loss 1.950, Val took 3290.527s\n",
      "Iter 200: Train loss 1.784, Learning Rate 1.000e-05, It/sec 0.062, Tokens/sec 61.712, Trained Tokens 178015, Peak mem 54.291 GB\n",
      "Iter 200: Saved adapter weights to llama-adapter-70/adapters.safetensors and llama-adapter-70/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 2.042, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.142, Trained Tokens 187323, Peak mem 60.731 GB\n",
      "Iter 220: Train loss 1.410, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.879, Trained Tokens 197656, Peak mem 60.731 GB\n",
      "Iter 230: Train loss 2.035, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.995, Trained Tokens 206503, Peak mem 60.731 GB\n",
      "Iter 240: Train loss 1.851, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.023, Trained Tokens 215302, Peak mem 60.731 GB\n",
      "Iter 250: Train loss 2.145, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.594, Trained Tokens 223637, Peak mem 60.731 GB\n",
      "Iter 260: Train loss 2.132, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.291, Trained Tokens 232086, Peak mem 60.731 GB\n",
      "Iter 270: Train loss 2.107, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.558, Trained Tokens 240721, Peak mem 60.731 GB\n",
      "Iter 280: Train loss 1.943, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.175, Trained Tokens 249334, Peak mem 60.731 GB\n",
      "Iter 290: Train loss 1.797, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 6.003, Trained Tokens 259593, Peak mem 60.731 GB\n",
      "Iter 300: Train loss 1.906, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.698, Trained Tokens 269080, Peak mem 60.731 GB\n",
      "Iter 300: Saved adapter weights to llama-adapter-70/adapters.safetensors and llama-adapter-70/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 1.902, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.581, Trained Tokens 278007, Peak mem 60.731 GB\n",
      "Iter 320: Train loss 1.870, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.828, Trained Tokens 287226, Peak mem 60.731 GB\n",
      "Iter 330: Train loss 2.078, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.291, Trained Tokens 295335, Peak mem 60.731 GB\n",
      "Iter 340: Train loss 1.684, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 6.189, Trained Tokens 305215, Peak mem 60.731 GB\n",
      "Iter 350: Train loss 2.236, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.375, Trained Tokens 313733, Peak mem 60.731 GB\n",
      "Iter 360: Train loss 1.824, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.177, Trained Tokens 321827, Peak mem 60.731 GB\n",
      "Iter 370: Train loss 1.680, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.991, Trained Tokens 331213, Peak mem 60.731 GB\n",
      "Iter 380: Train loss 2.088, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.265, Trained Tokens 339101, Peak mem 60.731 GB\n",
      "Iter 390: Train loss 1.744, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.351, Trained Tokens 347625, Peak mem 60.731 GB\n",
      "Iter 400: Val loss 1.977, Val took 3467.556s\n",
      "Iter 400: Train loss 1.859, Learning Rate 1.000e-05, It/sec 0.061, Tokens/sec 54.351, Trained Tokens 356582, Peak mem 60.731 GB\n",
      "Iter 400: Saved adapter weights to llama-adapter-70/adapters.safetensors and llama-adapter-70/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 2.150, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.233, Trained Tokens 364522, Peak mem 60.731 GB\n",
      "Iter 420: Train loss 2.079, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.231, Trained Tokens 372580, Peak mem 60.731 GB\n",
      "Iter 430: Train loss 1.763, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.610, Trained Tokens 381372, Peak mem 60.731 GB\n",
      "Iter 440: Train loss 2.156, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.226, Trained Tokens 389380, Peak mem 60.731 GB\n",
      "Iter 450: Train loss 1.748, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.733, Trained Tokens 398241, Peak mem 60.731 GB\n",
      "Iter 460: Train loss 2.016, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.441, Trained Tokens 406557, Peak mem 60.731 GB\n",
      "Iter 470: Train loss 2.129, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.571, Trained Tokens 415131, Peak mem 60.731 GB\n",
      "Iter 480: Train loss 1.958, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.795, Trained Tokens 424004, Peak mem 60.731 GB\n",
      "Iter 490: Train loss 1.942, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 4.974, Trained Tokens 431598, Peak mem 60.731 GB\n",
      "Iter 500: Train loss 2.088, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.985, Trained Tokens 440924, Peak mem 60.731 GB\n",
      "Iter 500: Saved adapter weights to llama-adapter-70/adapters.safetensors and llama-adapter-70/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 1.677, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.734, Trained Tokens 449919, Peak mem 60.731 GB\n",
      "Iter 520: Train loss 2.159, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 4.968, Trained Tokens 457688, Peak mem 60.731 GB\n",
      "Iter 530: Train loss 2.162, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.433, Trained Tokens 466115, Peak mem 60.731 GB\n",
      "Iter 540: Train loss 1.975, Learning Rate 1.000e-05, It/sec 0.006, Tokens/sec 5.565, Trained Tokens 474704, Peak mem 60.731 GB\n",
      "Iter 550: Train loss 2.296, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.383, Trained Tokens 482319, Peak mem 60.731 GB\n",
      "Iter 560: Train loss 2.121, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.376, Trained Tokens 490003, Peak mem 60.731 GB\n",
      "Iter 570: Train loss 1.847, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.747, Trained Tokens 499622, Peak mem 60.731 GB\n",
      "Iter 580: Train loss 1.975, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.571, Trained Tokens 507432, Peak mem 60.731 GB\n",
      "Iter 590: Train loss 2.191, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.805, Trained Tokens 515552, Peak mem 60.731 GB\n",
      "Iter 600: Val loss 1.807, Val took 3250.143s\n",
      "Iter 600: Train loss 1.898, Learning Rate 1.000e-05, It/sec 0.069, Tokens/sec 60.740, Trained Tokens 524363, Peak mem 60.731 GB\n",
      "Iter 600: Saved adapter weights to llama-adapter-70/adapters.safetensors and llama-adapter-70/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 1.719, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.682, Trained Tokens 534092, Peak mem 60.731 GB\n",
      "Iter 620: Train loss 1.731, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.334, Trained Tokens 543061, Peak mem 60.731 GB\n",
      "Iter 630: Train loss 2.049, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.814, Trained Tokens 551116, Peak mem 60.731 GB\n",
      "Iter 640: Train loss 1.941, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.000, Trained Tokens 559588, Peak mem 60.731 GB\n",
      "Iter 650: Train loss 1.835, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.550, Trained Tokens 569173, Peak mem 60.731 GB\n",
      "Iter 660: Train loss 1.986, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.037, Trained Tokens 577823, Peak mem 60.731 GB\n",
      "Iter 670: Train loss 2.053, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.076, Trained Tokens 586537, Peak mem 60.731 GB\n",
      "Iter 680: Train loss 1.796, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.177, Trained Tokens 595417, Peak mem 60.731 GB\n",
      "Iter 690: Train loss 2.034, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.241, Trained Tokens 604436, Peak mem 60.731 GB\n",
      "Iter 700: Train loss 2.074, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.602, Trained Tokens 612288, Peak mem 60.731 GB\n",
      "Iter 700: Saved adapter weights to llama-adapter-70/adapters.safetensors and llama-adapter-70/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 2.071, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.225, Trained Tokens 619599, Peak mem 60.731 GB\n",
      "Iter 720: Train loss 1.784, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.246, Trained Tokens 628534, Peak mem 60.731 GB\n",
      "Iter 730: Train loss 1.788, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.258, Trained Tokens 637606, Peak mem 60.731 GB\n",
      "Iter 740: Train loss 2.017, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.482, Trained Tokens 645336, Peak mem 60.731 GB\n",
      "Iter 750: Train loss 1.733, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.335, Trained Tokens 654374, Peak mem 60.731 GB\n",
      "Iter 760: Train loss 1.844, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.125, Trained Tokens 663335, Peak mem 60.731 GB\n",
      "Iter 770: Train loss 1.996, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.973, Trained Tokens 671886, Peak mem 60.731 GB\n",
      "Iter 780: Train loss 2.133, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.803, Trained Tokens 679998, Peak mem 60.731 GB\n",
      "Iter 790: Train loss 2.179, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.382, Trained Tokens 687537, Peak mem 60.731 GB\n",
      "Iter 800: Val loss 1.821, Val took 3215.935s\n",
      "Iter 800: Train loss 2.210, Learning Rate 1.000e-05, It/sec 0.072, Tokens/sec 54.846, Trained Tokens 695174, Peak mem 60.731 GB\n",
      "Iter 800: Saved adapter weights to llama-adapter-70/adapters.safetensors and llama-adapter-70/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 1.865, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.244, Trained Tokens 704359, Peak mem 60.731 GB\n",
      "Iter 820: Train loss 2.021, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.236, Trained Tokens 713317, Peak mem 60.731 GB\n",
      "Iter 830: Train loss 1.620, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 7.266, Trained Tokens 724085, Peak mem 60.731 GB\n",
      "Iter 840: Train loss 2.074, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.878, Trained Tokens 732392, Peak mem 60.731 GB\n",
      "Iter 850: Train loss 2.210, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.620, Trained Tokens 740252, Peak mem 60.731 GB\n",
      "Iter 860: Train loss 1.819, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.448, Trained Tokens 749590, Peak mem 60.731 GB\n",
      "Iter 870: Train loss 1.964, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.503, Trained Tokens 759206, Peak mem 60.731 GB\n",
      "Iter 880: Train loss 2.007, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.001, Trained Tokens 767713, Peak mem 60.731 GB\n",
      "Iter 890: Train loss 1.635, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.270, Trained Tokens 777010, Peak mem 60.731 GB\n",
      "Iter 900: Train loss 2.299, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.377, Trained Tokens 784650, Peak mem 60.731 GB\n",
      "Iter 900: Saved adapter weights to llama-adapter-70/adapters.safetensors and llama-adapter-70/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 1.933, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.658, Trained Tokens 792804, Peak mem 60.731 GB\n",
      "Iter 920: Train loss 1.668, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.712, Trained Tokens 802896, Peak mem 60.731 GB\n",
      "Iter 930: Train loss 1.849, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.849, Trained Tokens 811338, Peak mem 60.731 GB\n",
      "Iter 940: Train loss 1.709, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.851, Trained Tokens 821374, Peak mem 60.731 GB\n",
      "Iter 950: Train loss 2.231, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.060, Trained Tokens 829865, Peak mem 60.731 GB\n",
      "Iter 960: Train loss 1.720, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.442, Trained Tokens 839111, Peak mem 60.731 GB\n",
      "Iter 970: Train loss 1.854, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.978, Trained Tokens 847668, Peak mem 60.731 GB\n",
      "Iter 980: Train loss 1.649, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.821, Trained Tokens 857671, Peak mem 60.731 GB\n",
      "Iter 990: Train loss 1.729, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.002, Trained Tokens 866336, Peak mem 60.731 GB\n",
      "Iter 1000: Val loss 1.749, Val took 3258.004s\n",
      "Iter 1000: Train loss 1.906, Learning Rate 1.000e-05, It/sec 0.070, Tokens/sec 59.493, Trained Tokens 874826, Peak mem 60.731 GB\n",
      "Iter 1000: Saved adapter weights to llama-adapter-70/adapters.safetensors and llama-adapter-70/0001000_adapters.safetensors.\n",
      "Iter 1010: Train loss 1.982, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.186, Trained Tokens 883852, Peak mem 60.731 GB\n",
      "Iter 1020: Train loss 1.999, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.173, Trained Tokens 892796, Peak mem 60.731 GB\n",
      "Iter 1030: Train loss 1.826, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.222, Trained Tokens 901923, Peak mem 60.731 GB\n",
      "Iter 1040: Train loss 1.923, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.724, Trained Tokens 911808, Peak mem 60.731 GB\n",
      "Iter 1050: Train loss 1.996, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.918, Trained Tokens 920581, Peak mem 60.731 GB\n",
      "Iter 1060: Train loss 1.833, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.060, Trained Tokens 929369, Peak mem 60.731 GB\n",
      "Iter 1070: Train loss 1.823, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.624, Trained Tokens 937199, Peak mem 60.731 GB\n",
      "Iter 1080: Train loss 2.041, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.456, Trained Tokens 946414, Peak mem 60.731 GB\n",
      "Iter 1090: Train loss 1.926, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.478, Trained Tokens 955743, Peak mem 60.731 GB\n",
      "Iter 1100: Train loss 2.187, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.493, Trained Tokens 963350, Peak mem 60.731 GB\n",
      "Iter 1100: Saved adapter weights to llama-adapter-70/adapters.safetensors and llama-adapter-70/0001100_adapters.safetensors.\n",
      "Iter 1110: Train loss 1.887, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.314, Trained Tokens 972430, Peak mem 60.731 GB\n",
      "Iter 1120: Train loss 1.688, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.767, Trained Tokens 982305, Peak mem 60.731 GB\n",
      "Iter 1130: Train loss 1.898, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.336, Trained Tokens 991468, Peak mem 60.731 GB\n",
      "Iter 1140: Train loss 2.144, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.883, Trained Tokens 999886, Peak mem 60.731 GB\n",
      "Iter 1150: Train loss 1.918, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.644, Trained Tokens 1007942, Peak mem 60.731 GB\n",
      "Iter 1160: Train loss 2.050, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.951, Trained Tokens 1016345, Peak mem 60.731 GB\n",
      "Iter 1170: Train loss 1.961, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.125, Trained Tokens 1025134, Peak mem 60.731 GB\n",
      "Iter 1180: Train loss 1.733, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.568, Trained Tokens 1034688, Peak mem 60.731 GB\n",
      "Iter 1190: Train loss 1.700, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.319, Trained Tokens 1044014, Peak mem 60.731 GB\n",
      "Iter 1200: Val loss 1.922, Val took 3234.863s\n",
      "Iter 1200: Train loss 1.890, Learning Rate 1.000e-05, It/sec 0.071, Tokens/sec 61.539, Trained Tokens 1052664, Peak mem 60.731 GB\n",
      "Iter 1200: Saved adapter weights to llama-adapter-70/adapters.safetensors and llama-adapter-70/0001200_adapters.safetensors.\n",
      "Iter 1210: Train loss 2.004, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.925, Trained Tokens 1061204, Peak mem 60.731 GB\n",
      "Iter 1220: Train loss 1.973, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.074, Trained Tokens 1070053, Peak mem 60.731 GB\n",
      "Iter 1230: Train loss 1.805, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.600, Trained Tokens 1079705, Peak mem 60.731 GB\n",
      "Iter 1240: Train loss 1.531, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.776, Trained Tokens 1089681, Peak mem 60.731 GB\n",
      "Iter 1250: Train loss 1.694, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 7.069, Trained Tokens 1100196, Peak mem 60.731 GB\n",
      "Iter 1260: Train loss 1.894, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.334, Trained Tokens 1109244, Peak mem 60.731 GB\n",
      "Iter 1270: Train loss 1.514, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.799, Trained Tokens 1119253, Peak mem 60.731 GB\n",
      "Iter 1280: Train loss 2.081, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.810, Trained Tokens 1127817, Peak mem 60.731 GB\n",
      "Iter 1290: Train loss 1.882, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.477, Trained Tokens 1137610, Peak mem 60.731 GB\n",
      "Iter 1300: Train loss 1.839, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.207, Trained Tokens 1146776, Peak mem 60.731 GB\n",
      "Iter 1300: Saved adapter weights to llama-adapter-70/adapters.safetensors and llama-adapter-70/0001300_adapters.safetensors.\n",
      "Iter 1310: Train loss 1.855, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.232, Trained Tokens 1155680, Peak mem 60.731 GB\n",
      "Iter 1320: Train loss 2.015, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 6.097, Trained Tokens 1164433, Peak mem 60.731 GB\n",
      "Iter 1330: Train loss 2.209, Learning Rate 1.000e-05, It/sec 0.007, Tokens/sec 5.336, Trained Tokens 1171896, Peak mem 60.731 GB\n",
      "Iter 1337: Val loss 1.747, Val took 3225.924s\n",
      "Iter 1337: Train loss 2.124, Learning Rate 1.000e-05, It/sec 0.072, Tokens/sec 38.553, Trained Tokens 1177245, Peak mem 60.731 GB\n",
      "Saved final adapter weights to llama-adapter-70/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.lora \\\n",
    "  --train \\\n",
    "  --adapter-path llama-adapter-70 \\\n",
    "  --model mlx-community/Meta-Llama-3.1-70B-Instruct-4bit \\\n",
    "  --data new/splits \\\n",
    "  --batch-size 2 \\\n",
    "  --lora-layers 16 \\\n",
    "  --iters 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54645656-037f-40bb-ab3b-b2f721b91af0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 35) (324072859.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 34\u001b[0;36m\u001b[0m\n\u001b[0;31m    * Protein-protein interaction databases (e.g., STRING, BioGRID)\" \\\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 35)\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.generate \\\n",
    "  --adapter-path llama-adapter-70 \\\n",
    "  --model mlx-community/Meta-Llama-3.1-70B-Instruct-4bit \\\n",
    "    --prompt \" \\\n",
    "    -m 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f3fecac-1b5d-4789-bc9f-8e307041350c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|████████████████████████| 6/6 [00:00<00:00, 49152.00it/s]\n",
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are some use cases for graph genome?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Graph genome is a representation of a genome as a graph, where each node represents a sequence of nucleotides (DNA or RNA), and edges represent the relationships between these sequences. Here are some use cases for graph genome:\n",
      "\n",
      "1. **Read alignment and assembly**: Graph genome can be used to align short reads from high-throughput sequencing technologies to a reference genome. It can also be used to assemble these reads into a longer sequence. This is useful for identifying genetic variants, such as SNPs and indels, and for identifying novel genes or regulatory elements.\n",
      "2. **Variant detection and genotyping**: Graph genome can be used to detect genetic variants, such as SNPs, indels, and copy number variations (CNVs), by comparing the graph genome to a reference genome. This is useful for identifying genetic variants associated with disease, such as cancer or genetic disorders.\n",
      "3. **Chromosomal variation detection**: Graph genome can be used to detect chromosomal variations, such as translocations, inversions, and deletions, by comparing the graph genome to a reference genome. This is useful for identifying chromosomal abnormalities associated with disease, such as cancer or genetic disorders.\n",
      "4. **Phylogenetic analysis**: Graph genome can be used to infer the evolutionary relationships between different species by comparing their graph genomes. This is useful for understanding the evolutionary history of a species or group of species and for identifying potential areas of orthologous gene evolution.\n",
      "5. **Transcriptional regulation analysis**: Graph genome can be used to identify regulatory elements, such as promoters and enhancers, by comparing the graph genome to a reference genome. This is useful for understanding the transcriptional regulation of a gene or group of genes and for identifying potential areas of transcriptional regulation.\n",
      "6. **Cancer genome analysis**: Graph genome can be used to analyze the somatic mutations in cancer genomes, such as SNPs, indels, and CNVs. This is useful for identifying potential cancer driver genes and for developing targeted therapies.\n",
      "7. **Microbiome analysis**: Graph genome can be used to analyze the genomic content of microbial communities, such as the human microbiome. This\n",
      "==========\n",
      "Prompt: 142.493 tokens-per-sec\n",
      "Generation: 27.773 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.generate \\\n",
    "    --model mlx-community/Meta-Llama-3.1-8B-Instruct-4bit \\\n",
    "    --adapter-path llama-adapters33 \\\n",
    "    --prompt \"What are some use cases for graph genome?\" \\\n",
    "    -m 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a04cb6e3-5a1a-4304-8603-95ea3222cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|████████████████████████| 7/7 [00:00<00:00, 56899.47it/s]\n",
      "==========\n",
      "Prompt: <start_of_turn>user\n",
      "Tell me about recent research on the graph genome<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "Sure, here's a summary of recent research on the graph genome:\n",
      "* **Recent studies have identified and analyzed various types of genomic variations in the graph genome, including insertion and deletion mutations, gene duplications, and CRISPR-Cas9 CRISPR-Cas9 gene editing. These variations have been shown to play a role in the evolution of microbial genomes, including the ability to form new pathways and genes, which could be used to improve microbial production of biofuels or other useful chemicals.\n",
      "* **The graph genome is a complex and interconnected network of genetic variations that can be used to infer the functional relationships between genes and biochemical pathways.\n",
      "* **The graph genome is also a heterogeneous and mosaic network, with some regions being more densely populated than others. This heterogeneity can be used to infer the evolutionary relationships between genes and biochemical pathways, and the network structure can also be used to infer the evolutionary relationships between microbial genomes.\n",
      "* **The graph genome is a rapidly evolving field of research, with new discoveries being made all the time. The recent studies that have been conducted on the graph genome have shown that the graph genome is a complex and interconnected network that is more than just a collection of genes and biochemical pathways. The graph genome is also a dynamic network that is constantly evolving as new variations are discovered.\n",
      "* **The graph genome has the potential to be a powerful tool for understanding the evolution of microbial genomes and for developing new biotechnological applications. For example, the graph genome could be used to identify genes that are involved in the evolution of antibiotic resistance, or to develop new methods for producing biofuels. Furthermore, the graph genome could have a role in understanding the evolution of microbial genomes and identifying new targets for antibiotic resistance.\n",
      "* **The graph genome is a complex and interconnected network of genetic variations that can be used to infer the functional relationships between genes and biochemical pathways. The graph genome is also a heterogeneous and mosaic network, with some regions being more densely populated than others. The heterogeneity of the graph genome can be used to infer the evolutionary relationships between genes and biochemical pathways, and the network structure can also be used to infer the evolutionary relationships between microbial genomes.\n",
      "* **The graph genome is a rapidly evolving field of research, with new discoveries being made all the time. The recent studies that have been conducted on the graph genome have shown that the graph genome is a complex and interconnected network that is more than just a collection of genes and biochemical pathways. The graph genome is also a dynamic network that is constantly evolving as new variations are discovered.\n",
      "==========\n",
      "Prompt: 173.517 tokens-per-sec\n",
      "Generation: 52.777 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.generate \\\n",
    "    --model mlx-community/quantized-gemma-2b-it \\\n",
    "    --adapter-path adapters2 \\\n",
    "    --prompt \"Tell me about recent research on the graph genome\" \\\n",
    "    -m 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a36fbf-e04d-40e3-b9fd-4c277e9a1aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|███████████████████████| 7/7 [00:00<00:00, 132252.83it/s]\n",
      "==========\n",
      "Prompt: <start_of_turn>user\n",
      "What are some use cases for graph genome?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "**Use Cases for Graph Genome:**\n",
      "\n",
      "**1. Disease Network Discovery and Analysis:**\n",
      "* Identifying disease-disease relationships and pathways.\n",
      "* Discovering novel disease candidates.\n",
      "* Analyzing the spread of diseases and outbreaks.\n",
      "\n",
      "**2. Drug Target Identification:**\n",
      "* Identifying potential drug targets by identifying nodes that are frequently connected in the graph.\n",
      "* Predicting drug-target interactions.\n",
      "* Identifying novel therapeutic opportunities.\n",
      "\n",
      "**3. Gene Regulatory Network Analysis:**\n",
      "* Identifying key genes and regulatory networks.\n",
      "* Understanding the interactions between genes and proteins.\n",
      "* Studying the evolution of gene regulation.\n",
      "\n",
      "**4. Protein-Protein Interaction Network Analysis:**\n",
      "* Identifying significant protein-protein interactions.\n",
      "* Analyzing the structure and function of protein complexes.\n",
      "* Understanding the evolution of protein interactions.\n",
      "\n",
      "**5. Social Network Analysis:**\n",
      "* Identifying influential individuals and groups in a network.\n",
      "* Analyzing the structure and dynamics of social networks.\n",
      "* Studying the spread of information and rumors.\n",
      "\n",
      "**6. Network Biology and Complex Systems:**\n",
      "* Studying complex biological systems, such as the human brain, the immune system, and ecosystems.\n",
      "* Identifying network motifs and patterns.\n",
      "* Predicting network behavior and outcomes.\n",
      "\n",
      "**7. Network Medicine:**\n",
      "* Identifying disease subtypes and biomarkers.\n",
      "* Predicting disease outcomes and treatment responses.\n",
      "* Developing personalized medicine approaches.\n",
      "\n",
      "**8. Social Science and Humanities:**\n",
      "* Identifying influential individuals and communities.\n",
      "* Analyzing social networks and relationships.\n",
      "* Studying the spread of ideas and beliefs.\n",
      "\n",
      "**9. Engineering and Manufacturing:**\n",
      "* Identifying materials and components that interact with each other.\n",
      "* Designing and optimizing complex systems.\n",
      "* Discovering new designs and materials.\n",
      "\n",
      "**10. Other Fields:**\n",
      "* Studying political and economic networks.\n",
      "* Analyzing transportation and supply chain networks.\n",
      "* Identifying influential individuals in a network.\n",
      "==========\n",
      "Prompt: 54.847 tokens-per-sec\n",
      "Generation: 44.675 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "!mlx_lm.generate \\\n",
    "    --model mlx-community/quantized-gemma-2b-it \\\n",
    "    --prompt \"What are some use cases for graph genome?\" \\\n",
    "    -m 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9997b0-9a2f-42a6-9cd8-917fa5ee31c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595730c-91c4-43b1-be21-1239fc6f728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03fb423c-2c17-4734-9c02-d99248114cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidcs/git/imfine\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "078c871f-90db-49f2-b36b-ec28050913a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.jsonl test.jsonl   train.jsonl  valid.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11589bc5-d34b-4f74-9ab2-9053066baba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mlx-examples/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
